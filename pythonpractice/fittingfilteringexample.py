# -*- coding: utf-8 -*-
"""PL2_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Hqtgk7lBX0cu5IOcuke-GKG18kCan9l
"""

! pip install lightkurve

# Commented out IPython magic to ensure Python compatibility.
import lightkurve as kk
import numpy as np
import matplotlib.pyplot as plt
#force it back to standard plotting
# %matplotlib inline
#%matplotlib notebook

"""# Kepler Data: Fit and filter steps

In this section we will download a Kepler light curve and filter out the transits. The goal here is to normalize the continuum of the lightcurve so that we can stack the transits and get a clean folded transit profile.
"""

#pixelfile = kk.search_targetpixelfile(8462852, quarter=16).download(quality_bitmask='hardest');

pixelfile = kk.search_targetpixelfile('KIC6048106', quarter=list(np.arange(0,10, dtype=int))).download(quality_bitmask='hardest');

#downloads some specific time range worth of Kepler survey data
#pixelfile = kk.search_targetpixelfile('KIC3120320', quarter=16).download(quality_bitmask='hardest');

np.shape(pixelfile.flux)

i = pixelfile.flux[10, :, :]

plt.figure(figsize=(10,10))

plt.imshow(np.float64(i), cmap='gray', aspect='auto', origin='bottom')

lc = pixelfile.to_lightcurve(aperture_mask='all');

plt.figure(figsize=(15,10))

plt.subplot(1,2,1)
plt.plot(lc.time, lc.flux, '.-k')

plt.subplot(1,2,2)
plt.plot(lc.time, lc.flux, '.-k')

plt.xlim(1500, 1510)

# Crop light curve

t = []
f = []

t.append( lc.time[500:] )
f.append( lc.flux[500:] )

plt.figure(figsize=(15,10))

plt.subplot(1,2,1)
plt.plot(t[0], f[0], '.-k')

plt.subplot(1,2,2)
plt.plot(t[0], f[0], '.-k')

plt.xlim(1500, 1510)

! pip install csaps

# Fit with smoothing spline

from csaps import UnivariateCubicSmoothingSpline as ss

sf = []
# create upper & lower bounds, then filter anything that's out of that range
sf.append( ss(t[0], f[0], smooth=0.9)(t[0]) )

plt.figure(figsize=(15,10))

plt.subplot(1,2,1)
plt.plot(t[0], f[0], '.k', label='Orig LC data', alpha=0.5)
#plt.plot(t[-1], f[-1], '.-k', label='Filtered LC data')
for i in range(len(t) ):
  plt.plot(t[i], sf[i], '-', label='Fit ' + str(i))

plt.legend()

plt.subplot(1,2,2)
plt.plot(t[0], f[0], '.k', label='Orig LC data', alpha=0.5)
#plt.plot(t[-1], f[-1], '.-k', label='Filtered LC data')
for i in range(len(t) ):
  plt.plot(t[i], sf[i], '-', label='Fit ' + str(i))

plt.xlim(1520, 1530)

# Find difference of the fit and the data
df = np.abs(f[0] - sf[0])

# Find the standard deviation of the fit difference
sd = np.nanstd(df) * 3


plt.figure(figsize=(15,10))

plt.subplot(1,2,1)
plt.plot(t[-1], df, '.-k')
plt.plot(t[-1], sd + t[-1]*0, '-m')
plt.plot(t[-1][df > sd], df[df > sd], 'xr')

plt.subplot(1,2,2)
plt.plot(t[-1], df, '.-k')
plt.plot(t[-1], sd + t[-1]*0, '-m')
plt.plot(t[-1][df > sd], df[df > sd], 'xr')

plt.xlim(1520, 1530)

# Save filtered data into new variables
gi = df < sd

t.append( t[-1][gi] )
f.append( f[-1][gi] )

# Show what was filtered

plt.figure(figsize=(15,10))

plt.subplot(1,2,1)
plt.plot(t[-1], f[-1], '.-k')
plt.plot(t[-2][df > sd], f[-2][df > sd], 'xr')

plt.subplot(1,2,2)
plt.plot(t[1], f[1], '.-k')
plt.plot(t[-2][df > sd], f[-2][df > sd], 'xr')

plt.xlim(1500, 1510)

# Refit

sf.append( ss(t[1], f[1], smooth=0.9)(t[1]) )

plt.figure(figsize=(15,10))

plt.subplot(1,2,1)
plt.plot(t[0], f[0], '.k', label='Orig LC data', alpha=0.1)
plt.plot(t[-1], f[-1], '.-k', label='Filtered LC data')
for i in range(len(t)):
  plt.plot(t[i], sf[i], '-', label='Fit ' + str(i))

plt.legend()

plt.subplot(1,2,2)
plt.plot(t[0], f[0], '.k', label='Orig LC data', alpha=0.1)
plt.plot(t[-1], f[-1], '.-k', label='Filtered LC data')
for i in range(len(t)):
  plt.plot(t[i], sf[i], '-', label='Fit ' + str(i))

plt.xlim(1520, 1530)



"""# Kepler Data: Automated fit and filter

Here we will build a function that does everything in the last section.
"""

# remove transits
def rm_transits(t, f, sdm=3, sp=0.9):

  # Fit a smoothing spline to the data
  # We get a piecewise polynomial with this
  sf_pp = ss(t, f, smooth=sp)

  # Evaulate the fit at our time points
  sf = sf_pp(t)

  # Find difference of the fit and the data
  df = np.abs(f - sf)

  # Find the standard deviation of the fit difference
  # sdm = standard deviation multiplier
  sd = np.nanstd(df) * sdm

  # Save filtered data into new variables
  gi = df < sd

  tf = t[gi]
  ff = f[gi]

  return tf, ff, sf, sd, gi, sf_pp



#obj_name = 'KIC3120320'
#lc = kk.search_targetpixelfile(obj_name, quarter=16).download(quality_bitmask='hardest').to_lightcurve()


obj_name = 'KIC 6922244'
per = 3.5225
lc = kk.search_targetpixelfile(obj_name, quarter=4).download(quality_bitmask='hardest').to_lightcurve()



# Crop light curve

smooth = 0.9
sdm = 3.0

t = []
f = []
sf = []
gi = []

# original light curve data but cropping a bit
t.append( lc.time[500:] )
f.append( lc.flux[500:] )


for it in range(4):

  tf, ff, sf_tmp, sd, gi_tmp, sf_pp = rm_transits(t[-1], f[-1], sdm=sdm, sp=smooth)

  t.append(tf)
  f.append(ff)
  sf.append(sf_tmp)
  gi.append(gi_tmp)

len(t), len(sf), len(f)

plt.figure(figsize=(25,10))

plt.subplot(1,2,1)
plt.plot(t[0], f[0], '.-k', label='Orig LC data', alpha=0.1)
plt.plot(t[-1], f[-1], '.k', label='Filtered LC data')
for i in range(len(t) - 2):
  plt.plot(t[i], sf[i], '-', label='Fit ' + str(i))
plt.plot(t[i + 1], sf[i + 1], '-m', label='Fit ' + str(i + 1), linewidth=5, alpha=0.6)

plt.legend()

plt.subplot(1,2,2)
plt.plot(t[0], f[0], '.-k', label='Orig LC data', alpha=0.1)
plt.plot(t[-1], f[-1], '.k', label='Filtered LC data')
for i in range(len(t) - 2):
  plt.plot(t[i], sf[i], '-', label='Fit ' + str(i))
plt.plot(t[i + 1], sf[i + 1], '-m', label='Fit ' + str(i + 1), linewidth=5, alpha=0.6)

#plt.xlim(1520, 1530)
plt.xlim(t[0][0] + 20, t[0][0] + 30)

f_c = sf_pp(t[0])
f_n = f[0] / f_c

plt.figure(figsize=(25,10))

plt.subplot(2,1,1)
plt.title('Kepler light curve with continuum fit')
plt.plot(t[0], f[0], '.-k')
plt.plot(t[0], f_c, '-m', label='Continuum Fit', linewidth=5, alpha=0.8)

plt.legend()


plt.subplot(2,1,2)
plt.title('Normalized Kepler light curve')
plt.plot(t[0], f_n, '.-k')

# Fold the light curve


# Show fold without normalization
lc.fold(period=per).plot()

# Replace time with cropped time
lc.time = t[0]

# Replace flux with normalized flux
lc.flux = f[0] / f_c
# Replace flux error with cropped flux error
lc.flux_err = lc.flux_err[500:]

lc.fold(period=3.5225).plot()

obj_name



"""# CCF

In this section we will explore the cross correlation function. This function is mainly used to determine the shift between two similar datasets.
"""

# Lets create some data that looks like a spectrum with a couple spectral lines

offset = -2

t = np.arange(20)
f = np.zeros(20)
f2 = np.zeros(20)

f[10] = 1
f2[10 + -offset] = 1

#pfc = np.polyfit([0, 50, 100], [0, 1, 0], deg=2)

#f = np.polyval(pfc, t)
#f2 = np.polyval(pfc, t + offset)

from astropy.convolution import Gaussian1DKernel
k = Gaussian1DKernel(1.2).array
f = np.convolve(f, k, mode='same')
f2 = np.convolve(f2, k, mode='same')


plt.figure(figsize=(25,10))
plt.plot(t, f, '.-k')
plt.plot(t, f2, '.-r')

# Lets write a CCF from scratch

# Lets look at what the first few iterations of a CCF look like

a = np.copy(f) # Our first dataset

tb = t[3:-2]
b = f2[3:-2] # A cropped version of our second dataset


cfv, lags = [], []

for step in np.arange(-5, 2 + 1, dtype=int):

  # Crop the data to the range length of "b", but at a different index
  t_cropped = t[np.arange(len(b)) + step + 3]
  a_cropped = a[np.arange(len(b)) + step + 3]

  # Calcualte and store the correlation value
  cfv.append( np.sum(a_cropped * b) )

  # Store the step for plotting
  lags.append(step)

  # Plot to show the data shifting and being compared
  plt.figure(figsize=(15,2))
  plt.subplot(1,2,1)
  plt.plot(t, a, '.-k', alpha=0.3)
  plt.plot(tb + step, b, '.-r')
  plt.plot(t_cropped, a_cropped, '.-k')
  
  # Subplot to show the power and lags for the CCF
  plt.subplot(1,2,2)
  plt.plot(lags, cfv, '.-k')
  plt.plot(lags[-1], cfv[-1], '*m')



"""# Spectra: Normalize and CCF align"""

def normspec(t, f, sdm, smooth):

  #sdm = 3
  #smooth = 0.9

  tf = np.copy(t)
  ff = np.copy(f)
  for it in range(4):

    tf, ff, sf_tmp, sd, gi_tmp, sf_pp = rm_transits(tf, ff, sdm=sdm, sp=smooth)

  return f / sf_pp(t)

offset = 15

wl = np.linspace(5000, 5100, 500)
f = np.ones(500)
f2 = np.ones(500)

ll = np.random.randint(0, len(f) - offset, 30)
lf = np.random.uniform(0.001, 0.8, 30)
f[ll] = lf
f2[ll + offset] = lf


from astropy.convolution import Gaussian1DKernel
k = Gaussian1DKernel(0.9).array
f = np.convolve(f, k, mode='same')
f2 = np.convolve(f2, k, mode='same')

nc = np.polyval(np.polyfit([wl[0], np.mean([wl[0], np.mean(wl)]), np.mean(wl), wl[-1]], [1.3, 1.8, 2, 1], 4), wl)

f *= nc
f2 *= nc

f += np.random.normal(size=len(f)) / 50
f2 += np.random.normal(size=len(f)) / 80


#f *= 0.9

plt.figure(figsize=(25,10))
plt.plot(wl, f, '.-k')
plt.plot(wl, f2, '.-r')

fn = normspec(wl, f, 3, 0.001)
f2n = normspec(wl, f2, 3, 0.001)

plt.figure(figsize=(25,10))
plt.plot(wl, fn, '-k')
plt.plot(wl, f2n, '-r')

# Do cross corellations with the numpy CCF
ccfr = np.correlate(fn, f2n[30:-30], mode='same')

# Calculate what each lag is
dx = np.arange(len(f2)) - len(f2) / 2

# Only plot some of the data
plt_idx = np.abs(dx) < 20

plt.figure(figsize=(25,10))
plt.plot(dx[plt_idx], ccfr[plt_idx], '-k')
plt.xlabel('dX')
plt.ylabel('Correlation')

plt.figure(figsize=(25,10))
plt.plot(wl, fn, '-k')
plt.plot(wl - 15 * (wl[2] - wl[1]), f2n, '-r')

